{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in /Users/saadbazaz/opt/anaconda3/lib/python3.8/site-packages (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Game Emulation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "bg = None\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Selenium Browser\n",
    "\n",
    "# https://sites.google.com/a/chromium.org/chromedriver/home\n",
    "# Download ChromeDriver and extract. Then enter the fullpath here.\n",
    "PATH_TO_CHROME = r\"/Users/saadbazaz/Documents/FAST Studies/Digital Image Processing Lab/Project/Dino-Game-Hand-Gestures/t-rex-runner-gh-pages/chromedriver\"\n",
    "\n",
    "# The link to your Dino Game's index.html\n",
    "DINO_GAME_LINK = r\"/Users/saadbazaz/Documents/FAST Studies/Digital Image Processing Lab/Project/Dino-Game-Hand-Gestures/t-rex-runner-gh-pages/index.html\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(PATH_TO_CHROME)\n",
    "res = browser.get('file://' + DINO_GAME_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Selenium Action Chains\n",
    "\n",
    "actions = ActionChains(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "Separate the foreground (hand) from the background (everything else)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate main background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# To find the running average over the background\n",
    "#--------------------------------------------------\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate hand from segmented background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# To segment the region of hand in the image\n",
    "#---------------------------------------------\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-----------------\n",
    "# # MAIN FUNCTION\n",
    "# #-----------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # initialize weight for running average\n",
    "#     aWeight = 0.5\n",
    "\n",
    "#     # get the reference to the webcam\n",
    "#     camera = cv2.VideoCapture(0)\n",
    "\n",
    "#     # region of interest (ROI) coordinates\n",
    "#     top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "#     # initialize num of frames\n",
    "#     num_frames = 0\n",
    "\n",
    "#     # keep looping, until interrupted\n",
    "#     while(True):\n",
    "#         # get the current frame\n",
    "#         (grabbed, frame) = camera.read()\n",
    "\n",
    "#         # resize the frame\n",
    "#         frame = imutils.resize(frame, width=700)\n",
    "\n",
    "#         # flip the frame so that it is not the mirror view\n",
    "#         frame = cv2.flip(frame, 1)\n",
    "\n",
    "#         # clone the frame\n",
    "#         clone = frame.copy()\n",
    "\n",
    "#         # get the height and width of the frame\n",
    "#         (height, width) = frame.shape[:2]\n",
    "\n",
    "#         # get the ROI\n",
    "#         roi = frame[top:bottom, right:left]\n",
    "\n",
    "#         # convert the roi to grayscale and blur it\n",
    "#         gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "#         gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "#         # to get the background, keep looking till a threshold is reached\n",
    "#         # so that our running average model gets calibrated\n",
    "#         if num_frames < 30:\n",
    "#             run_avg(gray, aWeight)\n",
    "#         else:\n",
    "#             # segment the hand region\n",
    "#             hand = segment(gray)\n",
    "\n",
    "#             # check whether hand region is segmented\n",
    "#             if hand is not None:\n",
    "#                 # if yes, unpack the thresholded image and\n",
    "#                 # segmented region\n",
    "#                 (thresholded, segmented) = hand\n",
    "\n",
    "#                 # draw the segmented region and display the frame\n",
    "#                 cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "#                 cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "#         # draw the segmented hand\n",
    "#         cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "#         # increment the number of frames\n",
    "#         num_frames += 1\n",
    "\n",
    "#         # display the frame with segmented hand\n",
    "#         cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "#         # observe the keypress by the user\n",
    "#         keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "#         # if the user pressed \"q\", then stop looping\n",
    "#         if keypress == ord(\"q\"):\n",
    "#             break\n",
    "\n",
    "# # free up memory\n",
    "# camera.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognize Hands using fingers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Create a feature vector from the segmented hand region\n",
    "#--------------------------------------------------------------\n",
    "def create_feature_vector(thresholded, segmented):\n",
    "    # find the convex hull of the segmented hand region\n",
    "    chull = cv2.convexHull(segmented)\n",
    "\n",
    "    # find the most extreme points in the convex hull\n",
    "    extreme_top    = tuple(chull[chull[:, :, 1].argmin()][0])\n",
    "    extreme_bottom = tuple(chull[chull[:, :, 1].argmax()][0])\n",
    "    extreme_left   = tuple(chull[chull[:, :, 0].argmin()][0])\n",
    "    extreme_right  = tuple(chull[chull[:, :, 0].argmax()][0])\n",
    "\n",
    "    extreme_points = np.array([\n",
    "            extreme_top,\n",
    "            extreme_bottom,\n",
    "            extreme_left,\n",
    "            extreme_right\n",
    "        ])\n",
    "    \n",
    "    chull_mean = np.mean(extreme_points)\n",
    "    chull_std = np.std(extreme_points)\n",
    "\n",
    "    \n",
    "    \n",
    "    # find the center of the palm\n",
    "    cX = int((extreme_left[0] + extreme_right[0]) / 2)\n",
    "    cY = int((extreme_top[1] + extreme_bottom[1]) / 2)\n",
    "\n",
    "#     palm_center = cX, cY\n",
    "    \n",
    "    # find the maximum euclidean distance between the center of the palm\n",
    "    # and the most extreme points of the convex hull\n",
    "    distance = pairwise.euclidean_distances([(cX, cY)], Y=[extreme_left, extreme_right, extreme_top, extreme_bottom])[0]\n",
    "\n",
    "    maximum_distance = distance[distance.argmax()]\n",
    "    minimum_distance = distance[distance.argmin()]\n",
    "    distances_mean = np.mean(distance)\n",
    "    distances_std = np.std(distance)\n",
    "    \n",
    "#     print(type(distance))\n",
    "\n",
    "    # calculate the radius of the circle with 80% of the max euclidean distance obtained\n",
    "    radius = int(0.8 * maximum_distance)\n",
    "\n",
    "    # find the circumference of the circle\n",
    "    circumference = (2 * np.pi * radius)\n",
    "\n",
    "    # take out the circular region of interest which has \n",
    "    # the palm and the fingers\n",
    "    circular_roi = np.zeros(thresholded.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "    \n",
    "    # draw the circular ROI\n",
    "    cv2.circle(circular_roi, (cX, cY), radius, 255, 1)\n",
    "\n",
    "    # take bit-wise AND between thresholded hand using the circular ROI as the mask\n",
    "    # which gives the cuts obtained using mask on the thresholded hand image\n",
    "    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)\n",
    "\n",
    "    circular_roi_mean = np.mean(circular_roi)\n",
    "    circular_roi_std = np.std(circular_roi)\n",
    "    \n",
    "    \n",
    "    # compute the contours in the circular ROI\n",
    "    (cnts, _) = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "#     print (cnts)\n",
    "\n",
    "\n",
    "    cnts_mean = [np.mean(c) for c in cnts]\n",
    "    cnts_std = [np.std(c) for c in cnts]\n",
    "\n",
    "    \n",
    "    # initalize the finger count\n",
    "    count = 0\n",
    "\n",
    "    # loop through the contours found\n",
    "    for c in cnts:\n",
    "        # compute the bounding box of the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "        # increment the count of fingers only if -\n",
    "        # 1. The contour region is not the wrist (bottom area)\n",
    "        # 2. The number of points along the contour does not exceed\n",
    "        #     25% of the circumference of the circular ROI\n",
    "        if ((cY + (cY * 0.25)) > (y + h)) and ((circumference * 0.25) > c.shape[0]):\n",
    "            count += 1\n",
    "\n",
    "\n",
    "            \n",
    "#     print(\"chull_mean is \", type(chull_mean))\n",
    "#     print(\"chull_std is \", type(chull_std))\n",
    "#     print(\"maximum_distance is \", type(maximum_distance))\n",
    "#     print(\"minimum_distance is \", type(minimum_distance))\n",
    "#     print(\"distances_mean is \", type(distances_mean))\n",
    "#     print(\"distances_std is \", type(distances_std))\n",
    "#     print(\"radius is \", type(radius))\n",
    "#     print(\"circumference is \", type(circumference))\n",
    "#     print(\"circular_roi_mean is \", type(circular_roi_mean))\n",
    "#     print(\"circular_roi_std is \", type(circular_roi_std))\n",
    "#     print(\"cnts_mean is \", type(cnts_mean))\n",
    "#     print(\"cnts_std is \", type(cnts_std))\n",
    "#     print(\"count is \", type(count))           \n",
    "            \n",
    "            \n",
    "    feature_vector = np.array([\n",
    "        chull_mean,\n",
    "        chull_std,\n",
    "        maximum_distance,\n",
    "        minimum_distance,\n",
    "        distances_mean,\n",
    "        distances_std,\n",
    "        radius,\n",
    "        circumference,\n",
    "        circular_roi_mean,\n",
    "        circular_roi_std,\n",
    "        np.mean(cnts_mean),\n",
    "        np.mean(cnts_std),\n",
    "        count\n",
    "    ], dtype=np.float32)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment hand region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# To segment the region of hand in the image\n",
    "#---------------------------------------------\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Type of Contour: \" + str(type(segmented)))\n",
    "# print(\"Contour shape: \" + str(segmented.shape))\n",
    "# print(\"First 5 points in contour: \" + str(segmented[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the convex hull of the segmented hand region\n",
    "# chull = cv2.convexHull(segmented)\n",
    "\n",
    "# print(\"Type of Convex hull: \" + str(type(chull)))\n",
    "# print(\"Length of Convex hull: \" + str(len(chull)))\n",
    "# print(\"Shape of Convex hull: \" + str(chull.shape))\n",
    "\n",
    "# cv2.drawContours(image, [chull], -1, (0, 255, 0), 2)\n",
    "# cv2.imshow(\"Convex Hull\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train for Hand Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] please wait! calibrating...\n",
      "[STATUS] calibration successfull...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "recording = False\n",
    "training_data_feature_vectors = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # initialize accumulated weight\n",
    "    accumWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "\n",
    "    # calibration indicator\n",
    "    calibrated = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        # resize the frame\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # clone the frame\n",
    "        clone = frame.copy()\n",
    "\n",
    "        # get the height and width of the frame\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # convert the roi to grayscale and blur it\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # to get the background, keep looking till a threshold is reached\n",
    "        # so that our weighted average model gets calibrated\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, accumWeight)\n",
    "            if num_frames == 1:\n",
    "                print(\"[STATUS] please wait! calibrating...\")\n",
    "            elif num_frames == 29:\n",
    "                print(\"[STATUS] calibration successfull...\")\n",
    "        else:\n",
    "            \n",
    "            if recording:\n",
    "                # segment the hand region\n",
    "                hand = segment(gray)\n",
    "\n",
    "                # check whether hand region is segmented\n",
    "                if hand is not None:\n",
    "                    # if yes, unpack the thresholded image and\n",
    "                    # segmented region\n",
    "\n",
    "                    (thresholded, segmented) = hand\n",
    "\n",
    "    #                 print(\"Thresholded is: \", type(thresholded))\n",
    "    #                 print(\"Segmented is: \", type(segmented))\n",
    "\n",
    "#                     feature_vector = np.concatenate( (thresholded.flatten(), segmented.flatten()) )\n",
    "    #                 print (hand.flatten())\n",
    "\n",
    "    #                 print (\"FV is: \", feature_vector, \", dtype is:\", feature_vector.dtype, \", length is:\", feature_vector.size)\n",
    "\n",
    "                    # draw the segmented region and display the frame\n",
    "                    cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "\n",
    "                    # count the number of fingers\n",
    "                    feature_vector = create_feature_vector(thresholded, segmented)\n",
    "                    \n",
    "                    training_data_feature_vectors.append(feature_vector)\n",
    "\n",
    "                \n",
    "                # show the thresholded image\n",
    "                cv2.imshow(\"Thresholded\", thresholded)\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        # increment the number of frames\n",
    "        num_frames += 1\n",
    "\n",
    "        # display the frame with segmented hand\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "            \n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        elif keypress == ord(\"r\"):\n",
    "            \n",
    "            recording = not recording            \n",
    "            \n",
    "    \n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_feature_vectors = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # initialize accumulated weight\n",
    "    accumWeight = 0.5\n",
    "\n",
    "    # get the reference to the webcam\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # region of interest (ROI) coordinates\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # initialize num of frames\n",
    "    num_frames = 0\n",
    "\n",
    "    # calibration indicator\n",
    "    calibrated = False\n",
    "\n",
    "    # keep looping, until interrupted\n",
    "    while(True):\n",
    "        # get the current frame\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        # resize the frame\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # clone the frame\n",
    "        clone = frame.copy()\n",
    "\n",
    "        # get the height and width of the frame\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # convert the roi to grayscale and blur it\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # to get the background, keep looking till a threshold is reached\n",
    "        # so that our weighted average model gets calibrated\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, accumWeight)\n",
    "            if num_frames == 1:\n",
    "                print(\"[STATUS] please wait! calibrating...\")\n",
    "            elif num_frames == 29:\n",
    "                print(\"[STATUS] calibration successfull...\")\n",
    "        else:\n",
    "            \n",
    "            if recording:\n",
    "                # segment the hand region\n",
    "                hand = segment(gray)\n",
    "\n",
    "                # check whether hand region is segmented\n",
    "                if hand is not None:\n",
    "                    # if yes, unpack the thresholded image and\n",
    "                    # segmented region\n",
    "\n",
    "                    (thresholded, segmented) = hand\n",
    "\n",
    "    #                 print(\"Thresholded is: \", type(thresholded))\n",
    "    #                 print(\"Segmented is: \", type(segmented))\n",
    "\n",
    "#                     feature_vector = np.concatenate( (thresholded.flatten(), segmented.flatten()) )\n",
    "    #                 print (hand.flatten())\n",
    "\n",
    "    #                 print (\"FV is: \", feature_vector, \", dtype is:\", feature_vector.dtype, \", length is:\", feature_vector.size)\n",
    "\n",
    "                    # draw the segmented region and display the frame\n",
    "                    cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "\n",
    "                    # count the number of fingers\n",
    "                    feature_vector = count(thresholded, segmented)\n",
    "                    \n",
    "                    training_data_feature_vectors.append(feature_vector)\n",
    "\n",
    "                \n",
    "#         if results.right_hand_landmarks is not None:\n",
    "#             pos_y = results.right_hand_landmarks.landmark[10].y\n",
    "\n",
    "#             ## Draw threshold lines\n",
    "#             y_pix = int(np.multiply(lower_thresh, image.shape[0]))\n",
    "#             cv2.line(image, (0, y_pix), (image.shape[1],  y_pix), (0,255,0), thickness=2)\n",
    "            \n",
    "            \n",
    "#             y_pix = int(np.multiply(upper_thresh, image.shape[0]))\n",
    "#             cv2.line(image, (0, y_pix), (image.shape[1],  y_pix), (255,0,0), thickness=2)\n",
    "    \n",
    "\n",
    "                if fingers == 1:\n",
    "                    # print (\"Jump!\")\n",
    "\n",
    "\n",
    "                    if prev_pos == 1:\n",
    "                        pass  \n",
    "                    elif prev_pos == 2:\n",
    "                        actions = ActionChains(browser)                    \n",
    "                        actions.key_up(Keys.DOWN)\n",
    "                        actions.send_keys(Keys.SPACE)\n",
    "                        actions.perform() \n",
    "                    else:\n",
    "                        actions = ActionChains(browser)\n",
    "                        actions.send_keys(Keys.SPACE)\n",
    "                        actions.perform()                 \n",
    "\n",
    "                    prev_pos = 1\n",
    "\n",
    "                elif fingers == 2:\n",
    "                    # print (\"Duck.\")\n",
    "\n",
    "                    if prev_pos == 1:\n",
    "                        actions = ActionChains(browser)\n",
    "                        actions.key_down(Keys.DOWN)\n",
    "                        actions.perform()     \n",
    "                    elif prev_pos == 2:\n",
    "                        pass\n",
    "                    else:\n",
    "                        actions = ActionChains(browser)\n",
    "                        actions.key_down(Keys.DOWN)\n",
    "                        actions.perform()                     \n",
    "\n",
    "                    prev_pos = 2\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # print (\"Keep running.\")\n",
    "\n",
    "                    if prev_pos == 2:\n",
    "                        actions = ActionChains(browser)                    \n",
    "                        actions.key_up(Keys.DOWN)\n",
    "                        actions.perform() \n",
    "\n",
    "                    prev_pos = 0                   \n",
    "                \n",
    "                \n",
    "                cv2.putText(clone, str(fingers), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "                # show the thresholded image\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        # increment the number of frames\n",
    "        num_frames += 1\n",
    "\n",
    "        # display the frame with segmented hand\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "            \n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        elif keypress == ord(\"r\"):\n",
    "            recording = True            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
